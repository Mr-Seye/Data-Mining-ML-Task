{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96c4d05-c21f-4934-8a05-b6e3f74bb0a3",
   "metadata": {},
   "source": [
    "#Data Minining and Machine Learning\n",
    "##  Task 1 - Pre-processing and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a3b56-3d01-4096-959d-42724299caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-perameters\n",
    "# User can specify the number of features they\n",
    "# wish to prioritise.\n",
    "\n",
    "features = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb01bc-2bd0-45ff-9645-a9b8dd5f7c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming this is the first notebook viewed, \n",
    "# libraries are imported as required, so they may appear further down in blocks.\n",
    "# As of this comment there are no hyper-perameters i.e. number of features\n",
    "\n",
    "# This block loads the provided datasets and carries out data validation.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_data = np.load('x_train.npy')\n",
    "train_labels = np.load('y_train.npy')\n",
    "test_data = np.load('x_test.npy')\n",
    "test_labels = np.load('y_test.npy')\n",
    "\n",
    "missing_train_data = np.isnan(train_data)\n",
    "missing_train_labels = np.isnan(train_labels)\n",
    "missing_test_data = np.isnan(test_data)\n",
    "missing_test_labels = np.isnan(test_labels)\n",
    "\n",
    "missing_indices_traind = np.argwhere(missing_train_data)\n",
    "missing_indices_trainl = np.argwhere(missing_train_labels)\n",
    "missing_indices_testd = np.argwhere(missing_test_data)\n",
    "missing_indices_testl = np.argwhere(missing_test_labels)\n",
    "\n",
    "print(\"Missing values in the dataset:\")\n",
    "for row_idx, col_idx in missing_indices_testd:\n",
    "    print(f\"Row {row_idx}, Column {col_idx}\")\n",
    "\n",
    "print(\"Missing values in the dataset:\")\n",
    "for row_idx, col_idx in missing_indices_testl:\n",
    "    print(f\"Row {row_idx}, Column {col_idx}\")\n",
    "\n",
    "print(\"Missing values in the dataset:\")\n",
    "for row_idx, col_idx in missing_indices_traind:\n",
    "    print(f\"Row {row_idx}, Column {col_idx}\")\n",
    "\n",
    "print(\"Missing values in the dataset:\")\n",
    "for row_idx, col_idx in missing_indices_trainl:\n",
    "    print(f\"Row {row_idx}, Column {col_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c6678-73ea-4563-8427-9be9322c87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block performs the the preprocessing \n",
    "# and transformation.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "test_data_scaled = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59350564-05f6-46d9-819e-3ae93d758e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block calculates the Pearson Correlation coefficients for the \n",
    "# labels in the training sets and them sorts them by highest \n",
    "# absolute PCC value.\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "correlation_coefficients = [pearsonr(train_data_scaled[:, i], train_labels)[0] for i in range(train_data_scaled.shape[1])]\n",
    "\n",
    "top_indices = np.argsort(np.abs(correlation_coefficients))[-features:]\n",
    "top_coefficients = np.sort(np.abs(correlation_coefficients))[-features:]\n",
    "\n",
    "# Print top 10 feature indices (index of feature) and the absolute value of their coefficients\n",
    "print(f\"Top {features} feature indices:\", top_indices)\n",
    "print(f\"Top {features} feature coefficients:\", top_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062d3f81-8832-4812-a565-258278d100da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block defines a new training and test set based on the \n",
    "# identified top 10 features.\n",
    "# then initialises an SVC (an SVM) trains on the selected \n",
    "# features, makes predictions and produces an accuracy score.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "selected_train_data = train_data_scaled[:, top_indices]\n",
    "selected_test_data = test_data_scaled[:, top_indices]\n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(selected_train_data, train_labels)\n",
    "\n",
    "predictions = svm_classifier.predict(selected_test_data)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0233a78-b333-4888-a5e1-3ff40b2c41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block performs predition iterations using an \n",
    "# increasing number of features up to a defined maximum.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "# Iterate through different numbers of features\n",
    "for num_features in range(1, features + 1):\n",
    "    selected_train_data = train_data_scaled[:, top_indices[-num_features:]]\n",
    "    selected_test_data = test_data_scaled[:, top_indices[-num_features:]]\n",
    "\n",
    "    svm_classifier = SVC()\n",
    "    svm_classifier.fit(selected_train_data, train_labels)\n",
    "\n",
    "    predictions = svm_classifier.predict(selected_test_data)\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Accuracy using {num_features} feature(s): {100 * accuracy} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa13e1-5c85-452f-8810-15804ea79019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block plots the accuracies from the previous \n",
    "# block against the number of features.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, features + 1), accuracies, marker='o')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Number of Features')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053c9a5-58aa-402a-bbff-3badff02899d",
   "metadata": {},
   "source": [
    "# Markdown Question\n",
    "\n",
    "The difficulty in using pearson correlation method for feature selection is determining the appropriate number of features to select. The method described in the module to automatically choose the appropriate number of features will be computationally very expensive when the number of features is high. Describe an alternative method to reduce the computational complexity of the method discussed in class for datasets with large number of features. **Describe the method. No need to write the program**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc33af0-9251-4859-b24f-6efebdbd7b8d",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef46efb2-5530-47e0-a81f-97bbb2493deb",
   "metadata": {},
   "source": [
    "Particularly for high-dimensional datasets, dimensionality reduction techniques such as Principal Component Analysis (PCA) can lessen the computing load of feature selection based on Pearson correlation. \r\n",
    "\r\n",
    "PCA projects data onto a lower-dimensional space while preserving much of the variance in the data. \r\n",
    "\r\n",
    "The process consists of calculating the covariance matrix, picking primary components, performing eigenvalue decomposition, and projecting the data into the subspace that these components span. \r\n",
    "\r\n",
    "PCA makes feature selection more effective by lowering dimensionality, especially for datasets with a large number of features. \r\n",
    "\r\n",
    "To mitigate multicollinearity and processing expenses associated with high dimensionality, post-PCA feature selection based on Pearson correlation focuses on the reduced-dimensional dataset. \r\n",
    "\r\n",
    "To sum up, using PCA as a pre-processing step prior to feature selection based on Pearson correlation provides a workable way to deal with computing difficulties in high-dimensional datasets.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
